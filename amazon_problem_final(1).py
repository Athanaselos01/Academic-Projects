# -*- coding: utf-8 -*-
"""amazon-problem-final(1).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1KV69Ib4_JGgHv9KSFRJ0uNI0_3zJWwrU

# Case Study: Amazon Reviews

### Data Prep
"""

import pandas as pd
import numpy as np

# Read in the data

# Try reading only the first 100 rows to diagnose if the file is truncated or corrupted
df = pd.read_csv('Amazon_Unlocked_Mobile.csv', nrows=300)

# Sample the data to speed up computation
# Comment out this line to match with lecture
#df = df.sample(frac=0.1, random_state=10)

df.head()

# Drop missing values
df.dropna(inplace=True)

# Remove any 'neutral' ratings equal to 3
df = df[df['Rating'] != 3]

# Encode 4s and 5s as 1 (rated positively)
# Encode 1s and 2s as 0 (rated poorly)
df['Positively Rated'] = np.where(df['Rating'] > 3, 1, 0)
df.head(10)

from sklearn.model_selection import train_test_split

# Split data into training and test sets
X_train, X_test, y_train, y_test = train_test_split(df['Reviews'],
                                                    df['Positively Rated'],
                                                    random_state=0)

print('X_train first entry:\n\n', X_train.iloc[0])
print('\n\nX_train shape: ', X_train.shape)

"""# CountVectorizer"""

from sklearn.feature_extraction.text import CountVectorizer

# Fit the CountVectorizer to the training data
vect = CountVectorizer().fit(X_train)

vect

vect.get_feature_names_out()[::1000]

len(vect.get_feature_names_out())

# transform the documents in the training data to a document-term matrix
X_train_vectorized = vect.transform(X_train)

X_train_vectorized

X_train_vectorized[0]

from sklearn.linear_model import LogisticRegression

# Train the model
model = LogisticRegression()
model.fit(X_train_vectorized, y_train)

from sklearn.metrics import roc_auc_score

# Predict the transformed test documents
predictions = model.predict(vect.transform(X_test))

print('AUC: ', roc_auc_score(y_test, predictions))

# get the feature names as numpy array
feature_names = np.array(vect.get_feature_names_out())

# Sort the coefficients from the model
sorted_coef_index = model.coef_[0].argsort()

# Find the 10 smallest and 10 largest coefficients
# The 10 largest coefficients are being indexed using [:-11:-1]
# so the list returned is in order of largest to smallest
print('Smallest Coefs:\n{}\n'.format(feature_names[sorted_coef_index[:10]]))
print('Largest Coefs: \n{}'.format(feature_names[sorted_coef_index[:-11:-1]]))

"""# Tfidf"""

from sklearn.feature_extraction.text import TfidfVectorizer

# Fit the TfidfVectorizer to the training data specifiying a minimum document frequency of 5
vect = TfidfVectorizer(min_df=5).fit(X_train)
len(vect.get_feature_names_out())

X_train_vectorized = vect.transform(X_train)

model = LogisticRegression()
model.fit(X_train_vectorized, y_train)

predictions = model.predict(vect.transform(X_test))

print('AUC: ', roc_auc_score(y_test, predictions))

feature_names = np.array(vect.get_feature_names_out())

sorted_tfidf_index = X_train_vectorized.max(0).toarray()[0].argsort()

print('Smallest tfidf:\n{}\n'.format(feature_names[sorted_tfidf_index[:10]]))
print('Largest tfidf: \n{}'.format(feature_names[sorted_tfidf_index[:-11:-1]]))

sorted_coef_index = model.coef_[0].argsort()

print('Smallest Coefs:\n{}\n'.format(feature_names[sorted_coef_index[:10]]))
print('Largest Coefs: \n{}'.format(feature_names[sorted_coef_index[:-11:-1]]))

# These reviews are treated the same by our current model
print(model.predict(vect.transform(['not an issue, phone is working',
                                    'an issue, phone is not working'])))

"""# n-grams"""

# Fit the CountVectorizer to the training data specifiying a minimum
# document frequency of 5 and extracting 1-grams and 2-grams
vect = CountVectorizer(min_df=5, ngram_range=(1,2)).fit(X_train)

X_train_vectorized = vect.transform(X_train)

len(vect.get_feature_names_out())

model = LogisticRegression()
model.fit(X_train_vectorized, y_train)

predictions = model.predict(vect.transform(X_test))

print('AUC: ', roc_auc_score(y_test, predictions))

# These reviews are now correctly identified
print(model.predict(vect.transform(['not an issue, phone is working',
                                    'an issue, phone is not working'])))

feature_names = np.array(vect.get_feature_names_out())

sorted_coef_index = model.coef_[0].argsort()

print('Smallest Coefs:\n{}\n'.format(feature_names[sorted_coef_index[:10]]))
print('Largest Coefs: \n{}'.format(feature_names[sorted_coef_index[:-11:-1]]))

# Most ratings are positive
df['Positively Rated'].mean()

"""# ΧG_BOOST"""

import xgboost as xgb

from xgboost import XGBClassifier

x_train, x_test, y_train, y_test = train_test_split(df['Reviews'], df['Positively Rated'], test_size=0.2, random_state=42)

model = XGBClassifier()

from sklearn.feature_extraction.text import CountVectorizer

# Initialize a CountVectorizer with n-grams (1 and 2) and a minimum document frequency
# This is similar to the successful n-gram CountVectorizer used earlier
vectorizer_xgb = CountVectorizer(min_df=5, ngram_range=(1,2))

# Fit the vectorizer on the training text data and transform it
x_train_vectorized_xgb = vectorizer_xgb.fit_transform(x_train)

# Initialize and train the XGBoost classifier
model = XGBClassifier()
model.fit(x_train_vectorized_xgb, y_train)

print("Predictions:", predictions)

proba = model.predict_proba(vectorizer_xgb.transform(x_test))

print("Prediction probabilities:", proba)

from sklearn.metrics import roc_auc_score

auc_score = roc_auc_score(y_test, proba[:, 1])
print("AUC:", auc_score)

"""# test"""

import numpy as np
import pandas as pd
import re
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, f1_score
from sklearn.metrics.pairwise import cosine_similarity


!pip install gensim
import gensim.downloader as api
from gensim.models import Word2Vec


import torch
from transformers import BertTokenizer, BertModel, BertForSequenceClassification, Trainer, TrainingArguments

"""# Data Preprocessing"""

df = df[['Reviews', 'Positively Rated']].dropna()

def clean_text(text):
  text = text.lower()
  text = re.sub(r'[^a-z ]', '', text)
  return text

df['clean_text'] = df['Reviews'].apply(clean_text)

X_train, X_test, y_train, y_test = train_test_split(
    df['clean_text'], df['Positively Rated'], test_size=0.2, random_state=42
)

"""# Word2Vec"""

sentences = [text.split() for text in X_train]

w2v_model = Word2Vec(sentences=sentences, vector_size=100, window=5, min_count=2)

def document_embedding_w2v(text, model):
    vectors = [model.wv[w] for w in text.split() if w in model.wv]
    if len(vectors) == 0:
        return np.zeros(model.vector_size)
    return np.mean(vectors, axis=0)


X_train_vec = np.array([document_embedding_w2v(t, w2v_model) for t in X_train])
X_test_vec = np.array([document_embedding_w2v(t, w2v_model) for t in X_test])


clf_w2v = LogisticRegression(max_iter=1000)
clf_w2v.fit(X_train_vec, y_train)


y_pred_w2v = clf_w2v.predict(X_test_vec)


acc_w2v = accuracy_score(y_test, y_pred_w2v)
f1_w2v = f1_score(y_test, y_pred_w2v)

"""# GloVe"""

glove = api.load('glove-wiki-gigaword-50')

def document_embedding_glove(text, model):
    vectors = [model[w] for w in text.split() if w in model]
    if len(vectors) == 0:
        return np.zeros(50)
    return np.mean(vectors, axis=0)


X_train_glove = np.array([document_embedding_glove(t, glove) for t in X_train])
X_test_glove = np.array([document_embedding_glove(t, glove) for t in X_test])


clf_glove = LogisticRegression(max_iter=1000)
clf_glove.fit(X_train_glove, y_train)


y_pred_glove = clf_glove.predict(X_test_glove)


acc_glove = accuracy_score(y_test, y_pred_glove)
f1_glove = f1_score(y_test, y_pred_glove)

"""# BERT + ML"""

from sklearn.model_selection import train_test_split

X_train_small, _, y_train_small, _ = train_test_split(
    X_train, y_train, test_size=0.1, random_state=42, stratify=y_train
)

X_test_small, _, y_test_small, _ = train_test_split(
    X_test, y_test, test_size=0.1, random_state=42, stratify=y_test
)

tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
bert_model = BertModel.from_pretrained('bert-base-uncased')
bert_model.eval()

def bert_embedding(text):
    inputs = tokenizer(
        text,
        return_tensors='pt',
        truncation=True,
        padding=True,
        max_length=128
    )
    with torch.no_grad():
        outputs = bert_model(**inputs)
    return outputs.last_hidden_state[:, 0, :].squeeze().numpy()

X_train_bert = np.array([bert_embedding(t) for t in X_train_small])
X_test_bert = np.array([bert_embedding(t) for t in X_test_small])

# Train a classifier on BERT embeddings
clf_bert = LogisticRegression(max_iter=1000)
clf_bert.fit(X_train_bert, y_train_small)

# Make predictions and calculate metrics
y_pred_bert = clf_bert.predict(X_test_bert)
acc_bert = accuracy_score(y_test_small, y_pred_bert)
f1_bert = f1_score(y_test_small, y_pred_bert)

"""# BERT Fine-Tuning"""

class AmazonDataset(torch.utils.data.Dataset):
    def __init__(self, texts, labels):
        self.encodings = tokenizer(list(texts), truncation=True, padding=True, max_length=128)
        self.labels = list(labels)


    def __getitem__(self, idx):
        item = {k: torch.tensor(v[idx]) for k, v in self.encodings.items()}
        item['labels'] = torch.tensor(self.labels[idx])
        return item


    def __len__(self):
        return len(self.labels)


train_dataset = AmazonDataset(X_train, y_train)
test_dataset = AmazonDataset(X_test, y_test)


bert_ft = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)


training_args = TrainingArguments(
    output_dir='./results',
    num_train_epochs=2,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    eval_strategy='epoch',
    logging_dir='./logs',
    report_to="none" # Disable wandb logging
)


trainer = Trainer(
    model=bert_ft,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=test_dataset,
)


trainer.train()


predictions = trainer.predict(test_dataset)
y_pred_ft = np.argmax(predictions.predictions, axis=1)


acc_ft = accuracy_score(y_test, y_pred_ft)
f1_ft = f1_score(y_test, y_pred_ft)

"""# Σύγκριση Αποτελεσμάτων"""

results = pd.DataFrame({
'Method': ['Word2Vec', 'GloVe', 'BERT + ML', 'BERT Fine-Tuning'],
'Accuracy': [acc_w2v, acc_glove, acc_bert, acc_ft],
'F1-score': [f1_w2v, f1_glove, f1_bert, f1_ft]
})


results